{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import itertools\n",
    "from keras.models import *\n",
    "import random\n",
    "import sys\n",
    "from glob import glob\n",
    "import re\n",
    "import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import *\n",
    "import os\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Preprocessing\n",
    "\n",
    "### Data Format\n",
    "\n",
    "The [training data](https://research.fb.com/downloads/babi/) consists of a number of \"examples\". Each example contains a few memories, and the relevant question/answer pairs. Consider the following example\n",
    "\n",
    "1. Mary moved to the bathroom.\n",
    "2. John went to the hallway.\n",
    "3. Where is Mary?        bathroom        1\n",
    "4. Daniel went back to the hallway.\n",
    "5. Where is Daniel?     hallway  4\n",
    "\n",
    "In the above example, we have 3 memories and 2 question/answer pairs:\n",
    "\n",
    "- Three Memories\n",
    "    * Mary moved to the bathroom.\n",
    "    * John went to the hallway.\n",
    "    * Daniel went back to the hallway.\n",
    "    \n",
    "- 2 Questions/Answer Pairs:\n",
    "    * Q: *Where is Mary?* \n",
    "      A: bathroom\n",
    "    \n",
    "    * Q: *Where is Daniel?*\n",
    "      A: hallway\n",
    "      \n",
    " Note that for the first question, only memories (1, 2) are relevant. For the second question, memories (1, 2, 4) are relevant.\n",
    " \n",
    "\n",
    "### Abstractions\n",
    "* Corpus\n",
    ": Responsible for keeping tracks of the information related to the text corpus. For example, the total number of unique\n",
    "    words, mapping from string to text and so on.\n",
    "* ExampleParser\n",
    ": The training file consists of \"example\" units. ExampleParser takes a file with a number of examples and parses them into 3-tuples of (memories, question, answer)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        A corpus object maintains a mapping from a word (string) to a unique id (int).\n",
    "        '''\n",
    "        self.word_idx_dict = {}\n",
    "        self.uniq_word_cnt = 0\n",
    "    \n",
    "    def update_vocab(self, words):\n",
    "        '''\n",
    "        Updates the corpus with the given list of words. The words that are seen for the \n",
    "        first time are added to the word -> id dictionary.\n",
    "        '''\n",
    "        for word in words:\n",
    "            if word not in self.word_idx_dict:\n",
    "                self.word_idx_dict[word] = self.uniq_word_cnt\n",
    "                self.uniq_word_cnt += 1\n",
    "\n",
    "    def words_idx(self, words):\n",
    "        '''\n",
    "        Returns the list of IDs corresponding to the given words. \n",
    "        '''\n",
    "        return [self.word_idx_dict[word] for word in words]\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return [x.strip() for x in re.split('(\\W+)?', sentence) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExampleParser:\n",
    "    '''\n",
    "    Responsible for parsing examples for the babi tasks as specified at https://research.fb.com/downloads/babi/\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def add(example_lines, data, c):\n",
    "        '''\n",
    "        Takes the set of lines that form an example and:\n",
    "        a) updates the corpus with these lines \n",
    "        b) Parses the line to 3-tuples of the form: (memory, question, answer).\n",
    "        \n",
    "        A single story line may yield several 3-tuples of the above form. E.g.: \n",
    "        1 Mary moved to the bathroom.\n",
    "        2 John went to the hallway.\n",
    "        3 Where is Mary?        bathroom        1\n",
    "        4 Daniel went back to the hallway.\n",
    "        12 Where is Daniel?     hallway  4\n",
    "        \n",
    "        Will generate 2 tuples: \n",
    "        \n",
    "        - ([1 Mary moved to the bathroom., 2 John went to the hallway.], Where is Mary?, bathroom)\n",
    "        - ([1 Mary moved to the bathroom., 2 John went to the hallway., 4 Daniel went back to the hallway.],\n",
    "                                                                         Where is Daniel?, hallway)\n",
    "        \n",
    "        Note that instead of storing the actual words, an example stores the IDs of the associated words. A word -> ID\n",
    "        map is maintained in the corpus.\n",
    "        \n",
    "        data: List of 3-tuples (memories, question, answer), updated by \"add\".\n",
    "        c: The corpus object.\n",
    "        '''\n",
    "        memories = []\n",
    "        memories_txt = []\n",
    "        qa = []\n",
    "        for eg_line in example_lines:\n",
    "            if \"\\t\" not in eg_line: #normal memory\n",
    "                eg_line = c.tokenize(eg_line)\n",
    "                c.update_vocab(eg_line)\n",
    "                mem_id, memory = eg_line[0], c.words_idx(eg_line[1:])\n",
    "                memories.append(c.words_idx(eg_line))\n",
    "                memories_txt.append(eg_line)\n",
    "            else: #question line\n",
    "                ques, ans, hints = eg_line.split(\"\\t\")\n",
    "                ques = c.tokenize(ques)[1:]\n",
    "                c.update_vocab(ques)\n",
    "                ans = c.tokenize(ans)\n",
    "                c.update_vocab(ans)\n",
    "                data.append(([m for m in memories],\n",
    "                                  c.words_idx(ques), c.words_idx(ans)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_files(lines, corpus):\n",
    "        '''\n",
    "        Reads the given file, identifies splits of the example and adds them to th.\n",
    "        '''\n",
    "        data = []\n",
    "        eg_lines = [lines[0].decode('utf-8').strip()]\n",
    "        for line in lines[1:]:\n",
    "            line = line.decode('utf-8').strip()\n",
    "            if int(line.split(\" \", 1)[0]) == 1: #new story starts\n",
    "                ExampleParser.add(eg_lines, data, corpus)\n",
    "                eg_lines = [line.strip()]\n",
    "            else:\n",
    "                eg_lines.append(line.strip())\n",
    "        if len(eg_lines) > 0:\n",
    "            ExampleParser.add(eg_lines, data, corpus)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'1 Mary moved to the bathroom.\\n', b'2 John went to the hallway.\\n', b'3 Where is Mary? \\tbathroom\\t1\\n', b'4 Daniel went back to the hallway.\\n']\n",
      "[b'1 John travelled to the hallway.\\n', b'2 Mary journeyed to the bathroom.\\n', b'3 Where is John? \\thallway\\t1\\n', b'4 Daniel went back to the bathroom.\\n']\n"
     ]
    }
   ],
   "source": [
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "tar = tarfile.open(path)\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "test_lines = tar.extractfile(challenge.format('test')).readlines()\n",
    "train_lines = tar.extractfile(challenge.format('train')).readlines()\n",
    "print(train_lines[0:4])\n",
    "print(test_lines[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse datasets to (memories, question, answer) tuples, perform word -> idx mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amadaan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test files\n",
      "# training tuples = 10000\n",
      "# test tuples = 1000\n"
     ]
    }
   ],
   "source": [
    "c = Corpus()\n",
    "print(\"Processing training files\")\n",
    "train_tuples = ExampleParser.process_files(train_lines, c)\n",
    "print(\"Processing test files\")\n",
    "test_tuples = ExampleParser.process_files(test_lines, c)\n",
    "all_tuples = train_tuples + test_tuples\n",
    "print(\"# training tuples = {0}\\n# test tuples = {1}\".format(len(train_tuples), len(test_tuples)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Description\n",
    "- Let us look at the first example and the corresponding lines from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 3, 4, 10, 6]], [11, 12, 1, 13], [5])\n",
      "[b'1 Mary moved to the bathroom.\\n', b'2 John went to the hallway.\\n', b'3 Where is Mary? \\tbathroom\\t1\\n']\n"
     ]
    }
   ],
   "source": [
    "print(train_tuples[0])\n",
    "print(train_lines[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the mapping, note that \"1 Mary moved to the bathroom.\" has been mapped to \"0, 1, 2, 3, 4, 5, 6\". (line number is used as a feature).\n",
    "    - 1 -> 0\n",
    "    - Mary -> 1\n",
    "    - moved -> 2\n",
    "    - to -> 3\n",
    "    - the -> 4\n",
    "    - bathroom -> 5\n",
    "    - . (full stop) - 6\n",
    "\n",
    "* Similarily, 3 Where is Mary? is mapped to [11, 12, 1, 13] (note that Mary -> 1, the mapping is retained).\n",
    "\n",
    "* The answer is 5, bathroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset for Training\n",
    "\n",
    "- At this point, we have the 3-tuples of (memories, question, answer). \n",
    "\n",
    "- Different tuples may have different number of memories, and different number of words in memories and questions.\n",
    "\n",
    "- As a reminder, a 3-tuple contains a bunch of memories, 1 question and the corresponding answer. Also, we have processed these tuples so that at this point, they are just a bunch of numbers.\n",
    "\n",
    "- Because we want to use the same network for training, we will \"pad\" our 3-tuples to make them all of the same size. This means that:\n",
    "\n",
    "1. All the 3-tuples should have the same number of memories.\n",
    "2. All the memories should have the same number of words.\n",
    "3. All the questions should have the same number of words.\n",
    "\n",
    "\n",
    "- To achieve this, we will 0 pad the sequences. Let's find out relevant upper limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000, 31, 10, 8, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_memories = max([len(example[0]) for example in all_tuples])\n",
    "max_memory_len = max([len(memory) for example in all_tuples for memory in example[0]])\n",
    "max_ques_len = max([len(example[1]) for example in all_tuples])\n",
    "vocab_size = c.uniq_word_cnt\n",
    "len(train_tuples), len(test_tuples), c.uniq_word_cnt, max_num_memories, max_memory_len, max_ques_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Count |\n",
    "|---|---|\n",
    "| # Training Tuples | 10000 |\n",
    "| # Test Tuples | 1000 |\n",
    "| Max # words per memory | 8 |\n",
    "| Max # memories in a tuple | 10 |\n",
    "| Max # words per question | 4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_from_examples(examples, max_memory_len, max_num_memories, max_ques_len, vocab_size):\n",
    "    '''\n",
    "    Takes a number of examples, as well as measures required for 0 padding. \n",
    "    Returns a padded version of the memories, questions and the answer.\n",
    "    In other words, for each tuple, memories is now a matrix of:\n",
    "    a) max_num_memories * max_memory_len\n",
    "    b) question is an array with max_ques_len elements. The gaps are filled with 0s.\n",
    "    Also performs 1-hot encoding for the output.\n",
    "    '''\n",
    "    m, q, a = [], [], []\n",
    "    for (memories, ques, ans) in examples:\n",
    "        memories= pad_sequences(memories, maxlen=max_memory_len)\n",
    "        memories = np.concatenate([memories, np.zeros((max_num_memories - memories.shape[0],\n",
    "                                                       max_memory_len), 'int') ])\n",
    "        m.append(memories)\n",
    "        q.append(ques)\n",
    "        #ans_vec = np.zeros((vocab_size))\n",
    "        #ans_vec[ans] = 1\n",
    "        a.append(ans)\n",
    "    return np.array(m), pad_sequences(q, maxlen=max_ques_len), np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10, 8)\n",
      "(10000, 4)\n",
      "(10000, 1)\n",
      "(1000, 10, 8)\n",
      "(1000, 4)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m_train, q_train, a_train = get_dataset_from_examples(train_tuples,\n",
    "                                                     max_memory_len,\n",
    "                                                     max_num_memories,\n",
    "                                                     max_ques_len,\n",
    "                                                     vocab_size=c.uniq_word_cnt)\n",
    "\n",
    "m_test, q_test, a_test = get_dataset_from_examples(test_tuples,\n",
    "                                                     max_memory_len,\n",
    "                                                     max_num_memories,\n",
    "                                                     max_ques_len,\n",
    "                                                     vocab_size=c.uniq_word_cnt)\n",
    "\n",
    "print(m_train.shape) \n",
    "print(q_train.shape)\n",
    "print(a_train.shape)\n",
    "print(m_test.shape)\n",
    "print(q_test.shape)\n",
    "print(a_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will now build the model as specified at https://arxiv.org/abs/1503.08895.\n",
    "\n",
    "\n",
    "The relevant snippet from the paper are referenced inline. An attempt has been made to stay as close to the notation in paper as possible. Continuing from the notations used above, our network is designed to be trained on 1-tuple at a time. That is, the network will be fed a list of 3-tuples, where each of the 3-tuples is (memories, question, answer). \n",
    "\n",
    "For the entirety of the model building discussion, the focus will be on 1 such tuple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Encode the inputs\n",
    "\n",
    "The set x1, x2, ..., x_i that the paper mentions is our list of memories (each of the x_i is basically a list of numbers).\n",
    "\n",
    "![alt text](snippets\\m_i.png \"Input Encoding\")\n",
    "\n",
    "### m_i\n",
    "* For each of the inputs, x_i we learn an embedded representation, m_i. In the single fact example, the length of each x is 8. Each of the x is thus an 8-dimensional vectory. Similarily, the number of dimensions in the embedded space is 64. Thus, we want to map each of the 8 dim input vector to a 64 dim vector.\n",
    "\n",
    "* This is achieved by embedding each word of the input to a 64 dim vector, and then adding them all to give the 64 dim vector for the input.\n",
    "\n",
    "### Mapping m_1 to x_1: A complete example.\n",
    "\n",
    "* Suppose that x_1 is = \"1 Mary went to the kitchen\", mapped to [0 0 12 23 32 33 22 21] (0 padding to make the length = 8).\n",
    " For brevity, suppose that the number of hidden dimensions was **5** (it's 64 in the real case), then step 1 would be to learn 1 5 dimensional vector for each word in the sequence:\n",
    "\n",
    "- 0: '[0.13 0.69 0.52 -0.22 -1.15]'\n",
    "- 0: '[0.13 0.69 0.52 -0.22 -1.15]'\n",
    "- 1: '[0.05 0.23 -1.09 0.9 0.21]'\n",
    "- Mary: '[0.8 -1.09 -0.49 1.7 2.77]'\n",
    "- went: '[-0.95 0.87 0.88 0.02 0.09]'\n",
    "- to: '[0.73 -0.58 0.27 0.95 0.9]'\n",
    "- the: '[-1.13 0.41 -0.19 -1.41 0.07]'\n",
    "- kitchen: '[-0.5 , -1.11, -0.47,  0.09, -1.87]'\n",
    "\n",
    " The representation for the sentence is then just a sum of all of these: [-0.77, -0.18, -1.46,  3.19, -1.]).\n",
    " \n",
    " This is m_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(10), Dimension(8)]),\n",
       " TensorShape([Dimension(None), Dimension(10), Dimension(64)]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 64\n",
    "memories = Input(shape=(max_num_memories, max_memory_len))\n",
    "x = TimeDistributed(Embedding(input_dim=vocab_size, output_dim=n_hidden))(memories)\n",
    "m = Lambda(lambda xx: K.sum(xx, 2))(x)\n",
    "memories.shape, m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Encode the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](snippets\\u.png \"Query Encoding\")\n",
    "\n",
    "* Similarily, a query input is mapped from a 4 dim to a 64 dim space. This embedded representation of the query is called \"u\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(4)]),\n",
       " TensorShape([Dimension(None), Dimension(1), Dimension(64)]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Input(shape=(max_ques_len,))\n",
    "u = Embedding(input_dim=vocab_size, output_dim=n_hidden)(query)\n",
    "u = Lambda(lambda x : K.sum(x, 1))(u)\n",
    "u = Reshape(target_shape=(1, n_hidden))(u)\n",
    "query.shape, u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate p, the attention that each input deserves for the given query\n",
    "\n",
    "* Our answers need to be derived from our memories. Further, similarities in the query and memory is a good indicator of the memory containing the answer (because of presence of common words). The similarity is calculated by taking a dot product of the embedded query with each of the embedded input, followed by a softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](snippets\\p.png \"P\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10), Dimension(1)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = dot([m, u], axes=2)\n",
    "p = Reshape((max_num_memories,))(p)\n",
    "p = Activation(activation='softmax')(p)\n",
    "p = Reshape((max_num_memories,1))(p)\n",
    "p.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate c, the output embeddings\n",
    "\n",
    "* The input memory embeddings hopefully now have values that are conducive to finding out the relevant memories for a given question. We now want to learn another representation that will help in finding out the answer. The mechanics of finding out this embedding remain similar to what we did for m_i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](snippets\\c_i.png \"Output Representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10), Dimension(64)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TimeDistributed(Embedding(vocab_size, n_hidden))(memories)\n",
    "c = Lambda(lambda xx: K.sum(xx, 2))(x)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate o, the output representation\n",
    "\n",
    "* The output representation is calculated by taking a \"weighted average\" of the output memory repreesntation. Where the weights are given by p learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Representation, o\n",
    "![alt text](snippets/o.PNG \"Output Representation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reshape_14/Reshape:0' shape=(?, 1, 64) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = dot([c, p], axes=1)\n",
    "o = Reshape(target_shape=(1,n_hidden))(o)\n",
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Feed o + u to a softmax to get the answer\n",
    "![](snippets/a.png \"Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_3/Softmax:0' shape=(?, 31) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_in = Lambda(lambda ou: sum([ou[0], ou[1]]))([o, u])\n",
    "a_in = Reshape(target_shape=(n_hidden,))(a_in)\n",
    "answer = Dense(vocab_size, activation='softmax')(a_in)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babi_memmn = Model([memories, query], answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babi_memmn.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "1s - loss: 0.2716 - acc: 0.9073 - val_loss: 0.0022 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "1s - loss: 0.0341 - acc: 0.9909 - val_loss: 0.0570 - val_acc: 0.9870\n",
      "Epoch 3/5\n",
      "1s - loss: 0.0338 - acc: 0.9924 - val_loss: 0.0015 - val_acc: 0.9990\n",
      "Epoch 4/5\n",
      "1s - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0310 - val_acc: 0.9940\n",
      "Epoch 5/5\n",
      "1s - loss: 0.0224 - acc: 0.9958 - val_loss: 0.0034 - val_acc: 0.9990\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f467704e48>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(babi_memmn.optimizer.lr, 1e-2)\n",
    "params = {'verbose': 2, 'callbacks': [TQDMNotebookCallback(leave_inner=False)]}\n",
    "babi_memmn.fit([m_train, q_train], a_train, **params, batch_size=32, epochs=5,\n",
    "               validation_data=([m_test, q_test], a_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "3435d4f1b14e4919b7d445f8409c985d": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "381875f9345b408782719adbde7c09cd": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "5c0053fc4a3f42c4ad267e2680d70db4": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "61a441cedf904d45b2b114a6bd298068": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "6548670ecf8349819ac0629dc8f7c92c": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "698d5c6efefd496abf15b6f8d2ff7d63": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "cec359fc49914396b03e12df6846f400": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "d7a4412d8b674d358979e31652f949b5": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    },
    "fe1943ede490489698e3f0d357ce6cd0": {
     "views": [
      {
       "cell_index": 35
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
